{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2871781",
   "metadata": {},
   "source": [
    "# Parte 01: Portada y Entorno  \n",
    "\n",
    "## Magíster en Ciencia de Datos – UC  \n",
    "**Asignatura:** Aprendizaje Estadístico y Computacional  \n",
    "**Tarea:** Aprendizaje Supervisado y No Supervisado  \n",
    "\n",
    "\n",
    "## Objetivo de la Tarea  \n",
    "Aplicar técnicas de aprendizaje supervisado y no supervisado sobre un dataset real de clientes de tarjetas de crédito (UCI Credit Card Default), con el fin de:  \n",
    "\n",
    "- Predecir el riesgo de impago (default).  \n",
    "- Identificar segmentos de clientes según su comportamiento de pago.  \n",
    "- Integrar los hallazgos en un informe académico y un notebook documentado.  \n",
    "\n",
    "\n",
    "## Contexto Académico  \n",
    "- Tipo de actividad: Grupal  \n",
    "- Evaluación: Sumativa  \n",
    "- Ponderación: 20% de la asignatura  \n",
    "- Puntaje total: 60 puntos  \n",
    "- Exigencia mínima: 50% (nota mínima 4.0)  \n",
    "\n",
    "\n",
    "## Evaluación  \n",
    "- Revisión mediante notebook ejecutable y un informe en formato APA (2.500–7.500 palabras).  \n",
    "- Se evaluará:  \n",
    "  - Rigor metodológico (validación, resultados, reproducibilidad).  \n",
    "  - Interpretabilidad y aplicabilidad de los hallazgos en un contexto crediticio.  \n",
    "  - Cumplimiento de instrucciones formales (estructura, citas, anexos).  \n",
    "\n",
    "\n",
    "## Instrucciones Generales del Notebook  \n",
    "1. Responder cada apartado en orden, combinando explicaciones en Markdown y código Python ejecutable.  \n",
    "2. Mantener buenas prácticas: funciones reutilizables, pipelines y comentarios claros.  \n",
    "3. Justificar todas las decisiones técnicas (por ejemplo, manejo de desbalance o selección de K en clustering).  \n",
    "4. Incluir análisis ético y métricas de fairness en todos los apartados relevantes.  \n",
    "5. El notebook debe funcionar como guion técnico y narrativo, sirviendo de base para el informe académico.  \n",
    "\n",
    "\n",
    "## Estructura de Carpetas del Proyecto  \n",
    "- `/data/` → datasets utilizados.  \n",
    "- `/src/` → funciones reutilizables.  \n",
    "- `/reports/figures/` → visualizaciones e imágenes.  \n",
    "- `/reports/tables/` → tablas de resultados exportadas en `.csv`.  \n",
    "- `/models/` → artefactos entrenados (opcional).  \n",
    "\n",
    "Archivos adicionales:  \n",
    "- `requirements.txt` → dependencias de Python para reproducibilidad.  \n",
    "- `README.md` → instrucciones de ejecución.  \n",
    "\n",
    "\n",
    "## Entregables Finales  \n",
    "- Notebook ejecutable (`.ipynb`) con todos los apartados desarrollados.  \n",
    "- Informe académico en formato APA (2.500–7.500 palabras, sin código).  \n",
    "- Tablas y figuras exportadas desde el notebook.  \n",
    "\n",
    "\n",
    "## Checklist de Cumplimiento  \n",
    "- EDA completo: limpieza, calidad, distribuciones, correlaciones.  \n",
    "- Dos o más modelos supervisados con validación cruzada y métricas (PR-AUC, ROC, Recall, Precision, F1).  \n",
    "- Clustering justificado (Elbow, Silhouette y Davies-Bouldin) con descripción de perfiles.  \n",
    "- Fairness: desempeño por subgrupos (sexo, educación, estado civil).  \n",
    "- Conclusiones aplicables a un contexto crediticio real.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb3611",
   "metadata": {},
   "source": [
    "### Instrucciones\n",
    "- **EDA completo**: limpieza, calidad, outliers, subgrupos, correlaciones.  \n",
    "- **≥2 modelos supervisados**: validación cruzada, **PR-AUC** (principal), ROC-AUC, Recall, Precision, F1.  \n",
    "- **Clustering**: justificar **K** (Elbow + Silhouette + Davies-Bouldin), describir perfiles.  \n",
    "- **Fairness**: desempeño por subgrupos (sexo, educación, estado civil).  \n",
    "- **Conclusiones**: conectadas al problema real y a la aplicabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f684e82",
   "metadata": {},
   "source": [
    "## Parte 02 — Configuración del entorno y **reproducibilidad**\n",
    "\n",
    "**Qué haremos**  \n",
    "- Importar librerías base y crear la estructura de carpetas.  \n",
    "- Establecer **configuración de reproducibilidad** (semilla fija) para que particiones y resultados sean consistentes.\n",
    "\n",
    "**Por qué**  \n",
    "- Evita resultados distintos entre ejecuciones y facilita la revisión del docente.\n",
    "\n",
    "**Qué esperamos obtener**  \n",
    "- Mensajes de confirmación de versiones y rutas creadas (no hay métricas aún)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd8fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuración de reproducibilidad\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Estructura de carpetas\n",
    "BASE_DIR = Path().resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "FIG_DIR = BASE_DIR / \"reports\" / \"figures\"\n",
    "TAB_DIR = BASE_DIR / \"reports\" / \"tables\"\n",
    "for d in (DATA_DIR, FIG_DIR, TAB_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__, \"| Pandas:\", pd.__version__)\n",
    "print(\"Rutas listas ->\", \"DATA:\", DATA_DIR, \"| FIG:\", FIG_DIR, \"| TAB:\", TAB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eced7e2",
   "metadata": {},
   "source": [
    "## Parte 03 — Carga de datos y **comprobaciones iniciales**\n",
    "\n",
    "**Qué haremos**  \n",
    "- Cargar el CSV original desde `./data/credit_default.csv`.  \n",
    "- Verificar dimensiones, tipos y primeras filas.  \n",
    "- Definir columna objetivo `default.payment.next.month`.\n",
    "\n",
    "**Por qué**  \n",
    "- Confirmar que el archivo es el correcto y que las columnas esperadas existen.\n",
    "\n",
    "**Qué guardar para el informe**  \n",
    "- Ninguna figura aún; solo constatación de que el dataset se cargó correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = DATA_DIR / \"credit_default.csv\"\n",
    "assert csv_path.exists(), f\"Coloca el CSV en {csv_path} y vuelve a ejecutar.\"\n",
    "df = pd.read_csv(csv_path)\n",
    "target_col = \"default.payment.next.month\"\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae46201f",
   "metadata": {},
   "source": [
    "## Parte 04 — **Caracterización del dataset** (para el informe)\n",
    "\n",
    "**Fuente:** UCI Machine Learning Repository — *Default of Credit Card Clients (Taiwan, 2005)*.  \n",
    "**Tamaño:** ≈30.000 registros, **23 variables** + **target**.  \n",
    "**Estructura de variables:**  \n",
    "- **Demográficas:** `SEX`, `EDUCATION`, `MARRIAGE`, `AGE`  \n",
    "- **Financieras:** `LIMIT_BAL`  \n",
    "- **Comportamiento (6 meses previos):** `PAY_0–PAY_6`, `BILL_AMT1–6`, `PAY_AMT1–6`  \n",
    "- **Target:** `default.payment.next.month` (1=default, 0=no default)\n",
    "\n",
    "> **Notas para el informe:** es un dataset **abierto**, sin PII, usado como benchmark académico; limita su generalización a un país/periodo.\n",
    "\n",
    "*(Sin código en esta sección)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eeb4a1",
   "metadata": {},
   "source": [
    "## Parte 05 — Control de calidad y **limpieza mínima**\n",
    "\n",
    "**Qué haremos**  \n",
    "- Revisar duplicados y faltantes.  \n",
    "- Corregir **categorías inválidas**: `EDUCATION` (0/5/6→4 “otros”) y `MARRIAGE` (0→3 “otros”).\n",
    "\n",
    "**Por qué**  \n",
    "- Dejar la data **consistente** antes de EDA/modelado.  \n",
    "\n",
    "**Qué guardar**  \n",
    "- Tabla de faltantes (si existieran) para el anexo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicados y faltantes\n",
    "print(\"Duplicados:\", df.duplicated().sum())\n",
    "nulls = df.isna().sum().sort_values(ascending=False)\n",
    "if (nulls>0).any():\n",
    "    nulls.to_csv(TAB_DIR / \"eda_missing_count.csv\")\n",
    "    display(nulls[nulls>0].head(10))\n",
    "\n",
    "# Correcciones de categorías\n",
    "if \"EDUCATION\" in df.columns:\n",
    "    df[\"EDUCATION\"] = df[\"EDUCATION\"].replace({0:4, 5:4, 6:4})\n",
    "if \"MARRIAGE\" in df.columns:\n",
    "    df[\"MARRIAGE\"] = df[\"MARRIAGE\"].replace({0:3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebf553",
   "metadata": {},
   "source": [
    "## Parte 06 — **EDA** (Exploratory Data Analysis)\n",
    "\n",
    "**Qué haremos**  \n",
    "1. Medir **tasa global de default** y **desbalance**.  \n",
    "2. Estimar **tasas por subgrupos** (sexo, educación, matrimonio) y por **deciles de `LIMIT_BAL`**.  \n",
    "3. Calcular **correlaciones Spearman** para variables numéricas.  \n",
    "4. Guardar **tablas y figuras** para el informe.\n",
    "\n",
    "**Por qué**  \n",
    "- Entender el problema, detectar sesgos y elegir métricas adecuadas (**PR-AUC** para desbalance).\n",
    "\n",
    "**Qué guardar**  \n",
    "- `eda_target_rate.csv` · `eda_rate_by_[col].csv` · `eda_corr_spearman.csv` · `eda_corr_spearman.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c60ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Tasa global de default\n",
    "rate = df[target_col].mean()\n",
    "pd.DataFrame({\"target_rate\":[rate]}).to_csv(TAB_DIR / \"eda_target_rate.csv\", index=False)\n",
    "print(f\"Tasa global de default: {rate:.3f}\")\n",
    "\n",
    "# 2) Tasas por subgrupos\n",
    "def rate_by(col):\n",
    "    t = df.groupby(col)[target_col].mean().sort_values(ascending=False)\n",
    "    t.to_csv(TAB_DIR / f\"eda_rate_by_{col}.csv\")\n",
    "    return t\n",
    "\n",
    "for col in [\"SEX\",\"EDUCATION\",\"MARRIAGE\"]:\n",
    "    if col in df.columns:\n",
    "        print(f\"Tasa por {col}:\"); display(rate_by(col))\n",
    "\n",
    "# Deciles de LIMIT_BAL\n",
    "if \"LIMIT_BAL\" in df.columns:\n",
    "    df[\"_lim_decile\"] = pd.qcut(df[\"LIMIT_BAL\"], 10, labels=False, duplicates=\"drop\")\n",
    "    display(rate_by(\"_lim_decile\"))\n",
    "    df.drop(columns=[\"_lim_decile\"], inplace=True)\n",
    "\n",
    "# 3) Correlaciones Spearman\n",
    "num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "corr = df[num_cols].corr(method=\"spearman\")\n",
    "corr.to_csv(TAB_DIR / \"eda_corr_spearman.csv\")\n",
    "\n",
    "# 4) Figura (sin seaborn)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(corr, aspect=\"auto\")\n",
    "plt.colorbar(); plt.title(\"Matriz de correlaciones (Spearman)\")\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"eda_corr_spearman.png\"); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe978930",
   "metadata": {},
   "source": [
    "> **Interpretación esperada (completar tras ejecutar):**  \n",
    "> - El atraso reciente (`PAY_0`) debiese correlacionar fuertemente con el target.  \n",
    "> - `LIMIT_BAL` muestra gradiente de riesgo (mayor límite → menor default).  \n",
    "> - Los subgrupos pueden exhibir diferencias → se justifican métricas de **fairness** más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e77af",
   "metadata": {},
   "source": [
    "## Parte 07 — **Ingeniería de características** (features derivadas)\n",
    "\n",
    "**Qué haremos**  \n",
    "- Crear variables **estables** y explicables:  \n",
    "  - Tendencia de atraso: `pay_max`, `pay_mean`, `pay_count_late`.  \n",
    "  - Esfuerzo de pago: `pay_effort_mean` = promedio `PAY_AMT`/`BILL_AMT`.  \n",
    "  - Uso de línea: `utilization_mean` = promedio `BILL_AMT`/`LIMIT_BAL`.\n",
    "\n",
    "**Por qué**  \n",
    "- Capturan señales de comportamiento que **mejoran** el modelo y el clustering.\n",
    "\n",
    "**Qué guardar**  \n",
    "- `feat_corr_with_target.csv` (correlación de derivadas con el target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abada8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df.copy()\n",
    "\n",
    "pay_cols = [c for c in df.columns if c.startswith(\"PAY_\")]\n",
    "bill_cols = [c for c in df.columns if c.startswith(\"BILL_AMT\")]\n",
    "pamt_cols = [c for c in df.columns if c.startswith(\"PAY_AMT\")]\n",
    "\n",
    "# Tendencias de atraso\n",
    "if pay_cols:\n",
    "    df_feat[\"pay_max\"] = df[pay_cols].max(axis=1)\n",
    "    df_feat[\"pay_mean\"] = df[pay_cols].mean(axis=1)\n",
    "    df_feat[\"pay_count_late\"] = (df[pay_cols] > 0).sum(axis=1)\n",
    "\n",
    "# Esfuerzo de pago y uso de línea\n",
    "import numpy as np\n",
    "if bill_cols and pamt_cols:\n",
    "    ratios = []\n",
    "    for bcol, pcol in zip(sorted(bill_cols), sorted(pamt_cols)):\n",
    "        denom = df[bcol].replace(0, np.nan).abs()\n",
    "        ratios.append(df[pcol] / denom)\n",
    "    df_feat[\"pay_effort_mean\"] = pd.concat(ratios, axis=1).mean(axis=1, skipna=True)\n",
    "\n",
    "if \"LIMIT_BAL\" in df.columns and bill_cols:\n",
    "    df_feat[\"utilization_mean\"] = df[bill_cols].divide(df[\"LIMIT_BAL\"].replace(0, np.nan), axis=0).mean(axis=1, skipna=True)\n",
    "\n",
    "# Correlaciones de derivadas con el target\n",
    "derived = [c for c in [\"pay_max\",\"pay_mean\",\"pay_count_late\",\"pay_effort_mean\",\"utilization_mean\"] if c in df_feat.columns]\n",
    "if derived:\n",
    "    corr_deriv = df_feat[derived + [target_col]].corr(method=\"spearman\")[target_col].sort_values(ascending=False)\n",
    "    corr_deriv.to_csv(TAB_DIR / \"feat_corr_with_target.csv\")\n",
    "    display(corr_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f351e412",
   "metadata": {},
   "source": [
    "> **Buenas prácticas**  \n",
    "> - Evitar derivadas que usen información del **futuro** (fuga).  \n",
    "> - Documentar cálculo y **sentido de negocio** de cada feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d314ba83",
   "metadata": {},
   "source": [
    "## Parte 08 — Partición y **pipelines** (sin fuga de información)\n",
    "\n",
    "**Qué haremos**  \n",
    "- Split **train/test estratificado**.  \n",
    "- `ColumnTransformer` con imputación y escalado (numéricas) + one-hot (categóricas).  \n",
    "- Encapsular preprocesamiento en **Pipeline**.\n",
    "\n",
    "**Por qué**  \n",
    "- Los pasos de preparación deben aplicarse **solo** con datos de **train** durante la validación (evita fuga).\n",
    "\n",
    "**Qué guardar**  \n",
    "- No se exporta nada aquí; sí se imprimen formas y tasas del target en train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "y = df_feat[target_col].astype(int)\n",
    "X = df_feat.drop(columns=[target_col])\n",
    "\n",
    "num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "numeric_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())])\n",
    "categorical_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "\n",
    "preprocess = ColumnTransformer([(\"num\", numeric_pipe, num_cols), (\"cat\", categorical_pipe, cat_cols)], remainder=\"drop\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "\n",
    "print(\"Shapes train/test:\", X_train.shape, X_test.shape)\n",
    "print(\"Target rate train/test:\", round(y_train.mean(),3), round(y_test.mean(),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ca52d",
   "metadata": {},
   "source": [
    "## Parte 09 — **Modelado supervisado** (clasificación)\n",
    "\n",
    "**Qué haremos**  \n",
    "- Entrenar **Regresión Logística** y **Árbol de Decisión** con **CV k=5** (estratificada).  \n",
    "- Métrica principal: **PR-AUC** (adecuada con desbalance).  \n",
    "- Reportar también ROC-AUC, F1, Recall, Precision.  \n",
    "- Guardar métricas y curvas PR/ROC del **mejor modelo**.\n",
    "\n",
    "**Por qué**  \n",
    "- Comparar modelos de distinta naturaleza (lineal vs no lineal) y seleccionar el que mejor **recall en alta precisión** logre.\n",
    "\n",
    "**Qué guardar**  \n",
    "- `supervisado_metricas_test.csv` · figuras `roc_*.png` y `pr_*.png` · `cm_*_thr05.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714deca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# 1) Logistic Regression\n",
    "log_pipe = Pipeline([(\"prep\", preprocess), (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))])\n",
    "log_grid = GridSearchCV(log_pipe, {\"clf__C\":[0.1,1,10]}, scoring=\"average_precision\", cv=cv, n_jobs=-1)\n",
    "log_grid.fit(X_train, y_train)\n",
    "\n",
    "# 2) Decision Tree\n",
    "tree_pipe = Pipeline([(\"prep\", preprocess), (\"clf\", DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_SEED))])\n",
    "tree_grid = GridSearchCV(tree_pipe, {\"clf__max_depth\":[3,5,7,9], \"clf__min_samples_leaf\":[1,5,20]}, scoring=\"average_precision\", cv=cv, n_jobs=-1)\n",
    "tree_grid.fit(X_train, y_train)\n",
    "\n",
    "def eval_on_test(estimator, name):\n",
    "    proba = estimator.predict_proba(X_test)[:,1]\n",
    "    preds = (proba>=0.5).astype(int)\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"PR-AUC\": average_precision_score(y_test, proba),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, proba),\n",
    "        \"F1\": f1_score(y_test, preds),\n",
    "        \"Recall\": recall_score(y_test, preds),\n",
    "        \"Precision\": precision_score(y_test, preds)\n",
    "    }, proba\n",
    "\n",
    "log_metrics, log_proba = eval_on_test(log_grid.best_estimator_, \"LogisticRegression\")\n",
    "tree_metrics, tree_proba = eval_on_test(tree_grid.best_estimator_, \"DecisionTree\")\n",
    "\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame([log_metrics, tree_metrics]).sort_values(\"PR-AUC\", ascending=False)\n",
    "metrics_df.to_csv(TAB_DIR / \"supervisado_metricas_test.csv\", index=False)\n",
    "display(metrics_df)\n",
    "\n",
    "best_name = metrics_df.iloc[0][\"model\"]\n",
    "best_estimator = log_grid.best_estimator_ if best_name==\"LogisticRegression\" else tree_grid.best_estimator_\n",
    "\n",
    "RocCurveDisplay.from_estimator(best_estimator, X_test, y_test); plt.title(f\"ROC — {best_name}\")\n",
    "plt.savefig(FIG_DIR / f\"roc_{best_name}.png\"); plt.close()\n",
    "\n",
    "PrecisionRecallDisplay.from_estimator(best_estimator, X_test, y_test); plt.title(f\"PR — {best_name}\")\n",
    "plt.savefig(FIG_DIR / f\"pr_{best_name}.png\"); plt.close()\n",
    "\n",
    "cm = confusion_matrix(y_test, (best_estimator.predict_proba(X_test)[:,1] >= 0.5).astype(int))\n",
    "plt.figure(); plt.imshow(cm); plt.title(f\"Matriz de confusión — {best_name} (umbral=0.5)\"); plt.colorbar(); plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / f\"cm_{best_name}_thr05.png\"); plt.close()\n",
    "\n",
    "# Guardamos para fairness/umbral\n",
    "_best_proba = log_proba if best_name==\"LogisticRegression\" else tree_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da7719",
   "metadata": {},
   "source": [
    "## Parte 10 — **Optimización de umbral** (opcional recomendado)\n",
    "\n",
    "**Qué haremos**  \n",
    "- Buscar el **umbral de decisión** que maximiza **F1** (o la métrica/costo que definamos).  \n",
    "- Guardar la nueva matriz de confusión y métricas al umbral óptimo.\n",
    "\n",
    "**Por qué**  \n",
    "- En contextos desbalanceados, mover el umbral **mejora** el equilibrio Recall/Precision.\n",
    "\n",
    "**Qué guardar**  \n",
    "- `supervisado_metricas_umbral_opt.csv` · `cm_*_thr_opt.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ee064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def best_threshold(y_true, y_proba, metric=\"f1\"):\n",
    "    best_thr, best_val = 0.5, -1.0\n",
    "    for thr in np.linspace(0.05, 0.95, 19):\n",
    "        preds = (y_proba >= thr).astype(int)\n",
    "        val = f1_score(y_true, preds) if metric==\"f1\" else f1_score(y_true, preds)\n",
    "        if val > best_val:\n",
    "            best_thr, best_val = thr, val\n",
    "    return best_thr, best_val\n",
    "\n",
    "thr, val = best_threshold(y_test, _best_proba, metric=\"f1\")\n",
    "preds_opt = (_best_proba >= thr).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, preds_opt)\n",
    "plt.figure(); plt.imshow(cm); plt.title(f\"Matriz de confusión (umbral={thr:.2f})\"); plt.colorbar(); plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"cm_best_thr_opt.png\"); plt.close()\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"threshold\": thr,\n",
    "    \"F1\": f1_score(y_test, preds_opt),\n",
    "    \"Recall\": recall_score(y_test, preds_opt),\n",
    "    \"Precision\": precision_score(y_test, preds_opt)\n",
    "}]).to_csv(TAB_DIR / \"supervisado_metricas_umbral_opt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc43c58",
   "metadata": {},
   "source": [
    "## Parte 11 — **Fairness por subgrupos**\n",
    "\n",
    "**Qué haremos**  \n",
    "- Evaluar **Recall, Precision, FPR, FNR** por `SEX`, `EDUCATION`, `MARRIAGE` usando el **umbral óptimo**.\n",
    "\n",
    "**Por qué**  \n",
    "- Identificar disparidades en desempeño y documentarlas éticamente.\n",
    "\n",
    "**Qué guardar**  \n",
    "- `fairness_SEX.csv` · `fairness_EDUCATION.csv` · `fairness_MARRIAGE.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed0d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgroup_table(df_base, y_true, y_proba, subgroup_col, thr):\n",
    "    preds = (y_proba >= thr).astype(int)\n",
    "    rows = []\n",
    "    for g, idx in df_base.groupby(subgroup_col).groups.items():\n",
    "        yt = y_true.loc[idx]\n",
    "        pr = preds[idx]\n",
    "        fp = ((pr==1) & (yt==0)).sum()\n",
    "        fn = ((pr==0) & (yt==1)).sum()\n",
    "        tn = ((pr==0) & (yt==0)).sum()\n",
    "        tp = ((pr==1) & (yt==1)).sum()\n",
    "        rec = tp/(tp+fn) if (tp+fn)>0 else float(\"nan\")\n",
    "        pre = tp/(tp+fp) if (tp+fp)>0 else float(\"nan\")\n",
    "        fpr = fp/(fp+tn) if (fp+tn)>0 else float(\"nan\")\n",
    "        fnr = fn/(fn+tp) if (fn+tp)>0 else float(\"nan\")\n",
    "        rows.append({\"grupo\": g, \"n\": len(idx), \"Recall\": rec, \"Precision\": pre, \"FPR\": fpr, \"FNR\": fnr})\n",
    "    return pd.DataFrame(rows).sort_values(\"grupo\")\n",
    "\n",
    "thr = float(pd.read_csv(TAB_DIR / \"supervisado_metricas_umbral_opt.csv\")[\"threshold\"].iloc[0])\n",
    "\n",
    "for col in [\"SEX\",\"EDUCATION\",\"MARRIAGE\"]:\n",
    "    if col in df_feat.columns:\n",
    "        t = subgroup_table(df_feat, y_test, _best_proba, col, thr)\n",
    "        t.to_csv(TAB_DIR / f\"fairness_{col}.csv\", index=False)\n",
    "        print(f\"Fairness por {col}:\"); display(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659446d3",
   "metadata": {},
   "source": [
    "## Parte 12 — **Clustering** (no supervisado)\n",
    "\n",
    "**Qué haremos**  \n",
    "- KMeans con K=2..8 usando variables derivadas **estables**.  \n",
    "- Selección de K con **Elbow + Silhouette + Davies-Bouldin** y visualización **PCA 2D**.  \n",
    "- **Post-hoc**: relacionar clúster con tasa de default (el target no forma los clústeres).\n",
    "\n",
    "**Por qué**  \n",
    "- Caracterizar perfiles de riesgo para estrategias diferenciadas (no reemplaza al modelo supervisado).\n",
    "\n",
    "**Qué guardar**  \n",
    "- `clustering_k_metrics.csv` · `clusters_perfil.csv` · `clusters_default_rate.csv` · `clusters_pca2.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "cluster_vars = [c for c in [\"pay_mean\",\"pay_max\",\"pay_count_late\",\"pay_effort_mean\",\"utilization_mean\"] if c in df_feat.columns]\n",
    "assert cluster_vars, \"No hay variables derivadas para clustering (revisa Parte 07).\"\n",
    "\n",
    "Z = df_feat[cluster_vars].copy().astype(float).fillna(df_feat[cluster_vars].median())\n",
    "Z = StandardScaler().fit_transform(Z)\n",
    "\n",
    "rows = []\n",
    "for K in range(2, 9):\n",
    "    km = KMeans(n_clusters=K, random_state=RANDOM_SEED, n_init=10)\n",
    "    labels = km.fit_predict(Z)\n",
    "    sil = silhouette_score(Z, labels)\n",
    "    db  = davies_bouldin_score(Z, labels)\n",
    "    rows.append({\"K\":K, \"silhouette\":sil, \"davies_bouldin\":db, \"inertia\":km.inertia_})\n",
    "k_tbl = pd.DataFrame(rows)\n",
    "k_tbl.to_csv(TAB_DIR / \"clustering_k_metrics.csv\", index=False)\n",
    "display(k_tbl)\n",
    "\n",
    "# Selección sugerida (interpretación manda)\n",
    "K_best = int(k_tbl.sort_values([\"silhouette\",\"davies_bouldin\"], ascending=[False, True]).iloc[0][\"K\"])\n",
    "km = KMeans(n_clusters=K_best, random_state=RANDOM_SEED, n_init=10)\n",
    "labels = km.fit_predict(Z)\n",
    "df_feat[\"cluster\"] = labels\n",
    "\n",
    "perfil = df_feat.groupby(\"cluster\")[cluster_vars].agg([\"mean\",\"median\"]).round(3)\n",
    "perfil.to_csv(TAB_DIR / \"clusters_perfil.csv\")\n",
    "display(perfil)\n",
    "\n",
    "# PCA 2D\n",
    "pca2 = PCA(n_components=2, random_state=RANDOM_SEED).fit_transform(Z)\n",
    "pca_df = pd.DataFrame(pca2, columns=[\"PC1\",\"PC2\"]); pca_df[\"cluster\"] = labels\n",
    "pca_df.to_csv(TAB_DIR / \"clusters_pca2.csv\", index=False)\n",
    "\n",
    "plt.figure()\n",
    "for k in sorted(pca_df[\"cluster\"].unique()):\n",
    "    pts = pca_df[pca_df[\"cluster\"]==k]\n",
    "    plt.scatter(pts[\"PC1\"], pts[\"PC2\"], s=10, label=f\"C{k}\")\n",
    "plt.legend(); plt.title(\"PCA 2D — clusters\"); plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"clusters_pca2.png\"); plt.close()\n",
    "\n",
    "# Post-hoc: tasa de default por cluster\n",
    "t_default = df_feat.groupby(\"cluster\")[target_col].mean().rename(\"default_rate\").to_frame()\n",
    "t_default.to_csv(TAB_DIR / \"clusters_default_rate.csv\")\n",
    "display(t_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc423987",
   "metadata": {},
   "source": [
    "## Parte 13 — **Integración de resultados** y conclusiones\n",
    "\n",
    "**Qué haremos**  \n",
    "- Comparar **modelo ganador vs baseline** (mejora en PR-AUC y Recall).  \n",
    "- Resumir **segmentos/clústeres de riesgo** con tasa de default.  \n",
    "- Redactar **conclusiones y limitaciones** para el informe.\n",
    "\n",
    "**Qué guardar**  \n",
    "- `integracion_supervisado.csv` + tablas de clústeres ya generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c461de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sup_metrics = pd.read_csv(TAB_DIR / \"supervisado_metricas_test.csv\")\n",
    "best_row = sup_metrics.sort_values(\"PR-AUC\", ascending=False).iloc[0]\n",
    "baseline_row = sup_metrics.sort_values(\"PR-AUC\", ascending=False).iloc[-1]\n",
    "\n",
    "def pct_gain(a, b):\n",
    "    return (a - b) / b * 100 if b != 0 else float(\"nan\")\n",
    "\n",
    "comparativa = pd.DataFrame([{\n",
    "    \"metric\": \"PR-AUC\",\n",
    "    \"winner\": best_row[\"PR-AUC\"],\n",
    "    \"baseline\": baseline_row[\"PR-AUC\"],\n",
    "    \"gain_%\": pct_gain(best_row[\"PR-AUC\"], baseline_row[\"PR-AUC\"])\n",
    "},{\n",
    "    \"metric\": \"Recall\",\n",
    "    \"winner\": best_row[\"Recall\"],\n",
    "    \"baseline\": baseline_row[\"Recall\"],\n",
    "    \"gain_%\": pct_gain(best_row[\"Recall\"], baseline_row[\"Recall\"])\n",
    "}])\n",
    "comparativa.to_csv(TAB_DIR / \"integracion_supervisado.csv\", index=False)\n",
    "display(comparativa)\n",
    "\n",
    "# Mostrar tablas de clústeres si existen\n",
    "for p in [TAB_DIR / \"clusters_perfil.csv\", TAB_DIR / \"clusters_default_rate.csv\"]:\n",
    "    if p.exists():\n",
    "        print(\"Tabla disponible:\", p.name); display(pd.read_csv(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12266c78",
   "metadata": {},
   "source": [
    "### **Conclusiones (completar tras ejecutar)**\n",
    "- **Modelo ganador** y variables explicativas principales (mencionar señales: atraso reciente, uso de línea, etc.).  \n",
    "- **Segmentos de riesgo** a priorizar (alto atraso/baja capacidad vs bajo riesgo).  \n",
    "- **Limitaciones**: país/periodo único; variables acotadas; posible sesgo.  \n",
    "- **Trabajo futuro**: costos FP/FN, validación externa, features temporales avanzadas.\n",
    "\n",
    "> **Checklist antes de entregar**  \n",
    "> - [ ] Todas las tablas/figuras están en `/reports/`.  \n",
    "> - [ ] Se cita explícitamente la **fuente UCI**.  \n",
    "> - [ ] El informe APA usa estas tablas/figuras (sin pegar código)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
